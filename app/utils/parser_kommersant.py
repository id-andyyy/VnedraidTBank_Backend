import requests
from bs4 import BeautifulSoup
import logging
import time
import json
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
}


def get_news_data():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º requests –∏ BeautifulSoup.
    - –ó–∞—Ö–æ–¥–∏—Ç –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–µ–π.
    - –°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø–µ—Ä–≤—ã–µ 10 –Ω–æ–≤–æ—Å—Ç–µ–π.
    - –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ –∫–∞–∂–¥–æ–π —Å—Å—ã–ª–∫–µ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç.
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –ø–æ–ª–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º.
    """
    URL = "https://www.kommersant.ru/finance?from=main"
    news_data = []

    try:
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {URL}")
        response = requests.get(URL, headers=HEADERS, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ª–µ–Ω—Ç—ã
        news_articles = soup.find_all('article', class_='uho rubric_lenta__item js-article')
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_articles)} —Å—Ç–∞—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")

        if not news_articles:
            logger.warning("–ù–æ–≤–æ—Å—Ç–Ω–∞—è –ª–µ–Ω—Ç–∞ –ø—É—Å—Ç–∞. –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–Ω—Ç–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ (—á–µ—Ä–µ–∑ JS).")
            return []

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º—Å—è –ø–µ—Ä–≤—ã–º–∏ 10 –Ω–æ–≤–æ—Å—Ç—è–º–∏
        for i, article in enumerate(news_articles[:10]):
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Å—Ç–∞—Ç—å–∏ –∏–∑ data-–∞—Ç—Ä–∏–±—É—Ç–∞
                article_url = article.get('data-article-url')
                if not article_url:
                    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
                    title_link = article.find('a', class_='uho__link uho__link--overlay')
                    if title_link and title_link.get('href'):
                        article_url = f"https://www.kommersant.ru{title_link.get('href')}"
                    else:
                        logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω URL –¥–ª—è —Å—Ç–∞—Ç—å–∏ {i+1}")
                        continue

                logger.info(f"[{i+1}/10] –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏: {article_url}")

                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                article_response = requests.get(article_url, headers=HEADERS, timeout=10)
                article_response.raise_for_status()
                article_soup = BeautifulSoup(article_response.content, 'html.parser')

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_tag = article_soup.find('h1', class_='doc_header__name')
                title = title_tag.get_text(strip=True) if title_tag else "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ div —Å –∫–ª–∞—Å—Å–æ–º doc__body
                content_elem = article_soup.find('div', class_='doc__body')
                full_text = ""
                
                if content_elem:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                    text_paragraphs = content_elem.find_all('p', class_='doc__text')
                    content_parts = []
                    for p in text_paragraphs:
                        # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
                        text = p.get_text(strip=True)
                        if text:
                            content_parts.append(text)
                    full_text = "\n".join(content_parts)
                else:
                    full_text = "–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                if "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω" not in title and "–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω" not in full_text and full_text.strip():
                    news_data.append({
                        'title': title,
                        'full_text': full_text
                    })
                    logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–∞ —Å—Ç–∞—Ç—å—è: {title[:50]}...")
                else:
                    logger.warning(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å—é {i+1} - –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö")

            except requests.RequestException as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç–∞—Ç—å–∏ {i+1}: {e}")
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ {i+1}: {e}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(0.5)

    except requests.RequestException as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}")
    except Exception as e:
        logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")

    return news_data


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç (requests + bs4)...")
    all_news = get_news_data()
    
    if all_news:
        logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π.")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç —Å –∫–ª—é—á–∞–º–∏ "Name" –∏ "Description"
        output_data = [
            {"Name": news_item["title"], "Description": news_item["full_text"]}
            for news_item in all_news
        ]
        
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤ JSON –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        try:
            with open('kommersantNews.json', 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=4)
            logger.info("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª kommersantNews.json")
        except IOError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª kommersantNews.json: {e}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é –ø–µ—Ä–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        print("\nüóûÔ∏è  –ü—Ä–µ–≤—å—é —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π:")
        print("=" * 80)
        for i, news in enumerate(output_data[:3], 1):
            print(f"\n{i}. {news['Name']}")
            print(f"   üìù {news['Description'][:100]}...")
            print("-" * 80)
        
    else:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏. –í–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤.")


if __name__ == "__main__":
    main() 