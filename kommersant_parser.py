#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç (kommersant.ru)
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import logging
from urllib.parse import urljoin, urlparse
import csv


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('kommersant_parser.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class NewsItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
    id: str
    title: str
    url: str
    datetime: str
    rubric: str
    author: Optional[str] = None
    description: Optional[str] = None
    image_url: Optional[str] = None
    content: Optional[str] = None
    tags: Optional[List[str]] = None


class KommersantParser:
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç"""
    
    def __init__(self, delay: float = 1.0):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.base_url = "https://www.kommersant.ru"
        self.session = requests.Session()
        self.delay = delay
        
        # User-Agent –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })

    def _make_request(self, url: str) -> Optional[BeautifulSoup]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç BeautifulSoup –æ–±—ä–µ–∫—Ç
        
        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            BeautifulSoup –æ–±—ä–µ–∫—Ç –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            logger.info(f"–ó–∞–ø—Ä–æ—Å –∫: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            if response.encoding != 'utf-8':
                response.encoding = 'utf-8'
                
            soup = BeautifulSoup(response.text, 'html.parser')
            time.sleep(self.delay)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            return soup
            
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
            return None

    def parse_main_page(self, url: str = None) -> List[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –ª–µ–Ω—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–µ–π
        
        Args:
            url: URL –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ª–µ–Ω—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ NewsItem
        """
        if url is None:
            url = f"{self.base_url}/lenta?from=all_lenta"
            
        soup = self._make_request(url)
        if not soup:
            return []

        news_items = []
        
        # –ò—â–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –≤ –ª–µ–Ω—Ç–µ –Ω–æ–≤–æ—Å—Ç–µ–π
        articles = soup.find_all('article', class_='uho rubric_lenta__item js-article')
        
        for article in articles:
            try:
                news_item = self._parse_news_item_from_main(article)
                if news_item:
                    news_items.append(news_item)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏: {e}")
                continue

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        return news_items

    def _parse_news_item_from_main(self, article_elem) -> Optional[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç —ç–ª–µ–º–µ–Ω—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            article_elem: BeautifulSoup —ç–ª–µ–º–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏
            
        Returns:
            NewsItem –æ–±—ä–µ–∫—Ç –∏–ª–∏ None
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏ URL –∏–∑ data-–∞—Ç—Ä–∏–±—É—Ç–æ–≤
            doc_id = article_elem.get('data-article-docsid')
            article_url = article_elem.get('data-article-url')
            
            if not doc_id or not article_url:
                return None

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = article_elem.find('h2', class_='uho__name rubric_lenta__item_name')
            title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            time_elem = article_elem.find('p', class_='uho__tag rubric_lenta__item_tag')
            datetime_str = time_elem.get_text(strip=True) if time_elem else ""

            # –†—É–±—Ä–∏–∫–∞
            rubric_elem = article_elem.find('a', href=re.compile(r'/rubric/\d+'))
            rubric = rubric_elem.get_text(strip=True) if rubric_elem else "–ë–µ–∑ —Ä—É–±—Ä–∏–∫–∏"

            # –ê–≤—Ç–æ—Ä
            author_elem = article_elem.find('a', href=re.compile(r'/authors/\d+'))
            author = author_elem.get_text(strip=True) if author_elem else None

            # –¢–µ–≥–∏
            tag_elems = article_elem.find_all('a', class_='tag_list__link')
            tags = [tag.get_text(strip=True) for tag in tag_elems] if tag_elems else []

            # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_url = article_elem.get('data-article-image')

            return NewsItem(
                id=doc_id,
                title=title,
                url=article_url,
                datetime=datetime_str,
                rubric=rubric,
                author=author,
                image_url=image_url,
                tags=tags
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            return None

    def parse_article(self, url: str) -> Optional[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            NewsItem –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –∏–ª–∏ None
        """
        soup = self._make_request(url)
        if not soup:
            return None

        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ URL
            doc_id_match = re.search(r'/doc/(\d+)', url)
            doc_id = doc_id_match.group(1) if doc_id_match else str(hash(url))

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = soup.find('h1', class_='doc_header__name')
            title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            time_elem = soup.find('time', class_='doc_header__publish_time')
            datetime_str = ""
            if time_elem:
                datetime_attr = time_elem.get('datetime')
                if datetime_attr:
                    datetime_str = datetime_attr
                else:
                    datetime_str = time_elem.get_text(strip=True)

            # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
            content_elem = soup.find('div', class_='doc__body')
            content = ""
            if content_elem:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —Ç–µ–∫—Å—Ç–æ–º
                text_paragraphs = content_elem.find_all('p', class_='doc__text')
                content_parts = []
                for p in text_paragraphs:
                    # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
                    text = p.get_text(strip=True)
                    if text:
                        content_parts.append(text)
                content = "\n\n".join(content_parts)

            # –†—É–±—Ä–∏–∫–∞ –∏–∑ breadcrumbs –∏–ª–∏ meta
            rubric = "–ë–µ–∑ —Ä—É–±—Ä–∏–∫–∏"
            rubric_meta = soup.find('meta', attrs={'name': 'mywidget:category'})
            if rubric_meta:
                rubric = rubric_meta.get('content', '–ë–µ–∑ —Ä—É–±—Ä–∏–∫–∏')

            # –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑ meta
            description = ""
            desc_meta = soup.find('meta', attrs={'name': 'description'})
            if desc_meta:
                description = desc_meta.get('content', '')

            # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ Open Graph
            image_url = ""
            og_image = soup.find('meta', property='og:image')
            if og_image:
                image_url = og_image.get('content', '')

            return NewsItem(
                id=doc_id,
                title=title,
                url=url,
                datetime=datetime_str,
                rubric=rubric,
                description=description,
                image_url=image_url,
                content=content
            )

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return None

    def get_article_content(self, news_item: NewsItem) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ NewsItem
        
        Args:
            news_item: –û–±—ä–µ–∫—Ç NewsItem
            
        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None
        """
        full_article = self.parse_article(news_item.url)
        return full_article.content if full_article else None

    def parse_multiple_articles(self, urls: List[str]) -> List[NewsItem]:
        """
        –ü–∞—Ä—Å–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ç–µ–π
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ NewsItem
        """
        articles = []
        for url in urls:
            article = self.parse_article(url)
            if article:
                articles.append(article)
                
        return articles

    def search_news(self, query: str, limit: int = 20) -> List[NewsItem]:
        """
        –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        search_url = f"{self.base_url}/search"
        params = {
            'text': query,
            'sort': 'date'
        }
        
        try:
            response = self.session.get(search_url, params=params)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            results = []
            search_items = soup.find_all('article', class_='search_results__item')[:limit]
            
            for item in search_items:
                try:
                    link_elem = item.find('a', class_='search_results__item_name')
                    if link_elem:
                        article_url = urljoin(self.base_url, link_elem.get('href'))
                        article = self.parse_article(article_url)
                        if article:
                            results.append(article)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞: {e}")
                    continue
                    
            return results
            
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []

    def save_to_json(self, news_items: List[NewsItem], filename: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ JSON —Ñ–∞–π–ª
        
        Args:
            news_items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            filename: –ò–º—è —Ñ–∞–π–ª–∞
        """
        data = []
        for item in news_items:
            data.append({
                'id': item.id,
                'title': item.title,
                'url': item.url,
                'datetime': item.datetime,
                'rubric': item.rubric,
                'author': item.author,
                'description': item.description,
                'image_url': item.image_url,
                'content': item.content,
                'tags': item.tags
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {filename}")

    def save_to_csv(self, news_items: List[NewsItem], filename: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –≤ CSV —Ñ–∞–π–ª
        
        Args:
            news_items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            filename: –ò–º—è —Ñ–∞–π–ª–∞
        """
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['ID', '–ó–∞–≥–æ–ª–æ–≤–æ–∫', 'URL', '–î–∞—Ç–∞/–≤—Ä–µ–º—è', '–†—É–±—Ä–∏–∫–∞', '–ê–≤—Ç–æ—Ä', '–û–ø–∏—Å–∞–Ω–∏–µ', '–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'])
            
            for item in news_items:
                writer.writerow([
                    item.id,
                    item.title,
                    item.url,
                    item.datetime,
                    item.rubric,
                    item.author or '',
                    item.description or '',
                    item.content or ''
                ])
        
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {filename}")

    def get_rubric_news(self, rubric_id: int, limit: int = 50) -> List[NewsItem]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Ä—É–±—Ä–∏–∫–∏
        
        Args:
            rubric_id: ID —Ä—É–±—Ä–∏–∫–∏
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Ä—É–±—Ä–∏–∫–∏
        """
        rubric_url = f"{self.base_url}/rubric/{rubric_id}"
        soup = self._make_request(rubric_url)
        
        if not soup:
            return []
            
        news_items = []
        articles = soup.find_all('article', class_='uho rubric_lenta__item js-article')[:limit]
        
        for article in articles:
            news_item = self._parse_news_item_from_main(article)
            if news_item:
                news_items.append(news_item)
                
        return news_items


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π 1 —Å–µ–∫—É–Ω–¥–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
    parser = KommersantParser(delay=1.0)
    
    print("üóûÔ∏è  –ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ –ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç")
    print("=" * 50)
    
    # 1. –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    print("üì∞ –ü–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
    main_news = parser.parse_main_page()
    
    if main_news:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(main_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –Ω–æ–≤–æ—Å—Ç–µ–π
        print("\nüî• –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:")
        for i, news in enumerate(main_news[:5], 1):
            print(f"{i}. {news.title}")
            print(f"   üïê {news.datetime} | üìÇ {news.rubric}")
            if news.author:
                print(f"   ‚úçÔ∏è  {news.author}")
            print()
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        if main_news:
            print("üìñ –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏...")
            full_article = parser.parse_article(main_news[0].url)
            
            if full_article and full_article.content:
                print("‚úÖ –¢–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω:")
                print(f"üìù {full_article.content[:200]}...")
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏")
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª—ã
        parser.save_to_json(main_news, 'kommersant_news.json')
        parser.save_to_csv(main_news[:10], 'kommersant_news.csv')
        
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏")

    # 4. –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞
    print("\nüîç –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Å–ª–æ–≤—É '–ü—É—Ç–∏–Ω'...")
    search_results = parser.search_news('–ü—É—Ç–∏–Ω', limit=5)
    
    if search_results:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        for i, news in enumerate(search_results, 1):
            print(f"{i}. {news.title}")
    else:
        print("‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    # 5. –ü—Ä–∏–º–µ—Ä –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Ä—É–±—Ä–∏–∫–∏ "–ü–æ–ª–∏—Ç–∏–∫–∞" (ID: 1)
    print("\nüèõÔ∏è  –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Ä—É–±—Ä–∏–∫–∏ '–û–±—â–µ—Å—Ç–≤–æ'...")
    rubric_news = parser.get_rubric_news(7, limit=10)  # ID —Ä—É–±—Ä–∏–∫–∏ –û–±—â–µ—Å—Ç–≤–æ = 7
    
    if rubric_news:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(rubric_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ä—É–±—Ä–∏–∫–µ")
        for i, news in enumerate(rubric_news[:3], 1):
            print(f"{i}. {news.title}")
    else:
        print("‚ùå –ù–æ–≤–æ—Å—Ç–∏ —Ä—É–±—Ä–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")


if __name__ == "__main__":
    main() 